{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQnsucrJ-rbW",
        "outputId": "faa4cb10-c345-4a75-ea13-b1863a402423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq-NVmsz8hQv",
        "outputId": "169bcf38-f253-4faa-c0cc-4ff0839ded86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La inteligencia artificial transformará el futuro porque esta la suentre de la conseccionidad de de los dees públicos de años y empresa de recherche con un\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Definimos el pad_token (muy importante para evitar errores con padding)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_text = \"La inteligencia artificial transformará el futuro porque\"\n",
        "\n",
        "# Ahora tokenizamos con padding y atención sin errores\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generamos texto pasando la máscara de atención y pad_token_id\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    no_repeat_ngram_size=2,  # evita repetir bigramas exactos\n",
        "    temperature=1.0,         # controla aleatoriedad (1.0 es estándar)\n",
        "    top_k=50,                # limita a las 50 palabras más probables\n",
        "    top_p=0.95,              # permite al modelo escoger entre el top 95% de probabilidad acumulada\n",
        "    do_sample=True           # activa muestreo para variedad (en vez de greedy)\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n"
      ]
    }
  ]
}