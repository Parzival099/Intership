{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT00wBT_njaR",
        "outputId": "36528263-2842-4cc0-ac2c-e04025cf9261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7xsT6BySee",
        "outputId": "fb4d4dc9-3264-4940-c25d-7c909950ec5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingrese la ruta del archivo/content/CV.pdf\n",
            "Ingrese donde desea guardar el archivo.\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def extract_text(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def generate_dialogue_summary(text, num_sentences=4):\n",
        "    # Tokenización de oraciones\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Eliminación de palabras vacías (stopwords)\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    sentences = [sentence for sentence in sentences if sentence.strip() not in stop_words]\n",
        "\n",
        "    # Cálculo de la matriz TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Obtención de las importancias de las frases\n",
        "    sentence_importances = tfidf_matrix.sum(axis=1)\n",
        "\n",
        "    # Ordenamiento de las frases por importancia\n",
        "    ranked_sentences = sorted(((importance, sentence) for importance, sentence in zip(sentence_importances, sentences)), reverse=True)\n",
        "\n",
        "    # Selección de las primeras num_sentences frases más relevantes\n",
        "    top_sentences = [s[1] for s in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    # Crear diálogos combinando las frases seleccionadas\n",
        "    dialogue = []\n",
        "    for i in range(1, len(top_sentences), 2):\n",
        "        dialogue.append(top_sentences[i-1] + \" \" + top_sentences[i])\n",
        "\n",
        "    # Unir los diálogos en un solo texto\n",
        "    dialogue_text = \"\\n\".join(dialogue)\n",
        "\n",
        "    return dialogue_text\n",
        "\n",
        "# Extraer texto del PDF\n",
        "pdf_path = os.environ.get(\"pdf_path\", input(\"Ingrese la ruta del archivo\"))  # Ruta al archivo PDF\n",
        "output_file1 = input(\"Ingrese donde desea guardar el archivo\")\n",
        "output_file = output_file1 + \"/resumen.txt\"\n",
        "articulo_texto = extract_text(pdf_path)\n",
        "articulo_texto = articulo_texto.replace(\"[ edit ]\", \" \")\n",
        "\n",
        "# Generar resumen en forma de diálogo a partir de la importancia de las frases\n",
        "resumen_dialogo = generate_dialogue_summary(articulo_texto, num_sentences=4)\n",
        "\n",
        "# Guardar el resumen traducido en un archivo\n",
        "with open(output_file, \"w\") as file:\n",
        "    file.write(resumen_dialogo)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import discord\n",
        "from discord.ext import commands\n",
        "import os\n",
        "import PyPDF2\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "intents = discord.Intents.default()\n",
        "intents.typing = False\n",
        "intents.presences = False\n",
        "\n",
        "bot = commands.Bot(command_prefix='!', intents=intents)\n",
        "\n",
        "def extract_text(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def generate_dialogue_summary(text, num_sentences=4):\n",
        "    # Tokenización de oraciones\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Eliminación de palabras vacías (stopwords)\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    sentences = [sentence for sentence in sentences if sentence.strip() not in stop_words]\n",
        "\n",
        "    # Cálculo de la matriz TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Obtención de las importancias de las frases\n",
        "    sentence_importances = tfidf_matrix.sum(axis=1)\n",
        "\n",
        "    # Ordenamiento de las frases por importancia\n",
        "    ranked_sentences = sorted(((importance, sentence) for importance, sentence in zip(sentence_importances, sentences)), reverse=True)\n",
        "\n",
        "    # Selección de las primeras num_sentences frases más relevantes\n",
        "    top_sentences = [s[1] for s in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    # Crear diálogos combinando las frases seleccionadas\n",
        "    dialogue = []\n",
        "    for i in range(1, len(top_sentences), 2):\n",
        "        dialogue.append(top_sentences[i-1] + \" \" + top_sentences[i])\n",
        "\n",
        "    # Unir los diálogos en un solo texto\n",
        "    dialogue_text = \"\\n\".join(dialogue)\n",
        "\n",
        "    return dialogue_text\n",
        "\n",
        "@bot.event\n",
        "async def on_ready():\n",
        "    print(f'Logged in as {bot.user.name} - {bot.user.id}')\n",
        "\n",
        "@bot.command(name='resumir')\n",
        "async def resumir(ctx):\n",
        "    await ctx.send(\"¡Hola! Sube el archivo PDF que deseas resumir.\")\n",
        "\n",
        "    def check(message):\n",
        "        return message.author == ctx.author and message.attachments\n",
        "\n",
        "    try:\n",
        "        message = await bot.wait_for(\"message\", check=check, timeout=60)\n",
        "        attachment = message.attachments[0]\n",
        "        if not attachment.filename.endswith('.pdf'):\n",
        "            await ctx.send(\"El archivo adjunto debe ser un PDF.\")\n",
        "            return\n",
        "\n",
        "        file_data = await attachment.read()\n",
        "        pdf_path = os.environ.get(\"pdf_path\", input(\"Ingrese la ruta del archivo\"))  # Ruta al archivo PDF\n",
        "        output_file1 = input(\"Ingrese donde desea guardar el archivo\")\n",
        "        output_file = output_file1 + \"/resumen.txt\"\n",
        "        articulo_texto = extract_text(pdf_path)\n",
        "        articulo_texto = articulo_texto.replace(\"[ edit ]\", \" \")\n",
        "\n",
        "        # Generar resumen en forma de diálogo a partir de la importancia de las frases\n",
        "        resumen_dialogo = generate_dialogue_summary(articulo_texto, num_sentences=4)\n",
        "\n",
        "        # Guardar el resumen traducido en un archivo\n",
        "        with open(output_file, \"w\") as file:\n",
        "            file.write(resumen_dialogo)\n",
        "\n",
        "    except TimeoutError:\n",
        "        await ctx.send(\"Ha pasado mucho tiempo. Por favor, vuelve a ejecutar el comando y sube el archivo.\")\n",
        "\n",
        "TOKEN = os.environ.get(\"Your_Token\")\n",
        "bot.run(TOKEN)\n"
      ],
      "metadata": {
        "id": "bpldLs37ld4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
